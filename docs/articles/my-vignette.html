<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Convex Optimization by convexjlr • convexjlr</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cerulean/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">convexjlr</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="..//index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/my-vignette.html">Convex Optimization by convexjlr</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/Non-Contradiction/convexjlr">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Convex Optimization by convexjlr</h1>
                        <h4 class="author">Changcheng Li</h4>
            
            <h4 class="date">2017-10-01</h4>
          </div>

    
    
<div class="contents">
<p>The aim of package <code>convexjlr</code> is to provide optimization results rapidly and reliably in <code>R</code> once you formulate your problem as a convex problem. Having this in mind, we write this vignette in a problem-oriented style. The vignette will walk you through several examples using package <code>convexjlr</code>:</p>
<ul>
<li>Lasso;</li>
<li>Logistic regression;</li>
<li>Support Vector Machine (SVM);</li>
<li>Smallest circle covering multiple points.</li>
</ul>
<p>Although these problems already have mature solutions, the purpose here is to show the wide application of convex optimization and how you can use <code>convexjlr</code> to deal with them easily and extendably.</p>
<p>Some of the examples here are of statistics nature (like Lasso and logistic regression), and some of the examples here are of machine-learning nature (like SVM), they may be appealing to readers with certain backgrounds. If you don’t know either of this, don’t be afraid, the smallest circle problem requires no certain background knowledge.</p>
<p>We hope you can get ideas for how to use <code>convexjlr</code> to solve your own problems by reading these examples. If you would like to share your experience on using <code>convexjlr</code>, don’t hesitate to contact me: <a href="mailto:cxl508@psu.edu">cxl508@psu.edu</a>.</p>
<p>Knowledge for convex optimization is not neccessary for using <code>convexjlr</code>, but it will help you a lot in formulating convex optimization problems and in using <code>convexjlr</code>.</p>
<ul>
<li>
<a href="https://en.wikipedia.org/wiki/Convex_optimization">Wikipedia page for convex optimization</a> is a good starting point.</li>
<li>
<a href="http://dcp.stanford.edu/">The page for Disciplined Convex Programming</a> can teach you more about DCP, which is the basis for <code>Convex.jl</code> and thus <code>convexjlr</code>.</li>
<li>
<a href="https://github.com/JuliaOpt/Convex.jl">Github page for <code>Convex.jl</code></a> can give you more imformation for <code>Convex.jl</code>, which <code>convexjlr</code> is built upon.</li>
</ul>
<p>To use package <code>convexjlr</code>, we first need to attach it and do some initial setup:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(convexjlr)</code></pre></div>
<pre><code>## 
## 载入程辑包：'convexjlr'</code></pre>
<pre><code>## The following object is masked from 'package:base':
## 
##     norm</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="../reference/convex_setup.html">convex_setup</a></span>(<span class="dt">backend =</span> <span class="st">"JuliaCall"</span>)</code></pre></div>
<pre><code>## Doing initialization. It may take some time. Please wait.</code></pre>
<pre><code>## Julia at location /Applications/Julia-0.6.app/Contents/Resources/julia/bin will be used.</code></pre>
<pre><code>## Julia version 0.6.0 found.</code></pre>
<pre><code>## Julia initiation...</code></pre>
<pre><code>## Finish Julia initiation.</code></pre>
<pre><code>## Loading setup script for JuliaCall...</code></pre>
<pre><code>## Finish loading setup script for JuliaCall.</code></pre>
<pre><code>## [1] TRUE</code></pre>
<div id="lasso" class="section level2">
<h2 class="hasAnchor">
<a href="#lasso" class="anchor"></a>Lasso</h2>
<p>Lasso is a variable selection and coefficient estimation method for linear regression. Interested reader can see <a href="http://statweb.stanford.edu/~tibs/lasso.html">the Lasso Page</a> and the Wikipedia page <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Lasso (statistics)</a> for more information.</p>
<p>Let us first see the <code>lasso</code> function using <code>convexjlr</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lasso &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, lambda){
    p &lt;-<span class="st"> </span><span class="kw">ncol</span>(x)
    ## n is a scalar, you don't have to use J(.) to send it to Julia.
    n &lt;-<span class="st"> </span><span class="kw">nrow</span>(x) ## n &lt;- J(nrow(x))
    ## lambda is a scalar, you don't have to use J(.) to send it to Julia.
    ## lambda &lt;- J(lambda)
    ## x is a matrix and y is a vector, you have to use J(.) to send them to Julia.
    x &lt;-<span class="st"> </span><span class="kw"><a href="../reference/J.html">J</a></span>(x)
    y &lt;-<span class="st"> </span><span class="kw"><a href="../reference/J.html">J</a></span>(y)
    ## coefficient vector beta and intercept b.
    beta &lt;-<span class="st"> </span><span class="kw"><a href="../reference/variable_creating.html">Variable</a></span>(p)
    b &lt;-<span class="st"> </span><span class="kw"><a href="../reference/variable_creating.html">Variable</a></span>()
    ## MSE is mean square error.
    MSE &lt;-<span class="st"> </span><span class="kw"><a href="../reference/Expr.html">Expr</a></span>(<span class="kw"><a href="../reference/sumsquares.html">sumsquares</a></span>(y <span class="op">-</span><span class="st"> </span>x <span class="op">%*%</span><span class="st"> </span>beta <span class="op">-</span><span class="st"> </span>b) <span class="op">/</span><span class="st"> </span>n)
    ## the L-1 penalty term of Lasso.
    penalty &lt;-<span class="st"> </span><span class="kw"><a href="../reference/Expr.html">Expr</a></span>(lambda <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw">abs</span>(beta)))
    ## In Lasso, we want to minimize the sum of MSE and penalty.
    p1 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/problem_creating.html">minimize</a></span>(MSE <span class="op">+</span><span class="st"> </span>penalty)
    <span class="kw"><a href="../reference/cvx_optim.html">cvx_optim</a></span>(p1)
    <span class="kw">list</span>(<span class="dt">coef =</span> <span class="kw"><a href="../reference/value.html">value</a></span>(beta), <span class="dt">intercept =</span> <span class="kw"><a href="../reference/value.html">value</a></span>(b))
}</code></pre></div>
<p>In the function, <code>x</code> is the predictor matrix, <code>y</code> is the response we have, <code>lambda</code> is the positive tuning parameter which controls the sparsity of the estimation. And the <code>lasso</code> function will return the coefficient and intercept solved by <code>cvx_optim</code>.</p>
<p>Now we can see a little example using the <code>lasso</code> function we have just built.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">1000</span>
p &lt;-<span class="st"> </span><span class="dv">100</span>
## Sigma, the covariance matrix of x, is of AR-1 strcture.
Sigma &lt;-<span class="st"> </span><span class="kw">outer</span>(<span class="dv">1</span><span class="op">:</span>p, <span class="dv">1</span><span class="op">:</span>p, <span class="cf">function</span>(i, j) <span class="fl">0.5</span> <span class="op">^</span><span class="st"> </span><span class="kw">abs</span>(i <span class="op">-</span><span class="st"> </span>j))
x &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>p), n, p) <span class="op">%*%</span><span class="st"> </span><span class="kw">chol</span>(Sigma)
## The real coefficient is all zero except the first, second and fourth elements.
beta0 &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, p)
beta0[<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>)
y &lt;-<span class="st"> </span>x <span class="op">%*%</span><span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span><span class="fl">0.2</span> <span class="op">*</span><span class="st"> </span><span class="kw">rnorm</span>(n)

betahat &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">lasso</span>(x, y, <span class="fl">0.5</span>)<span class="op">$</span>coef, <span class="dv">4</span>)
betahat[<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>]</code></pre></div>
<pre><code>## [1] 4.8063 0.8879 0.0000 1.8072</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## You can see that the rest elements of betahat are all zero.
<span class="kw">all</span>(betahat[<span class="dv">5</span><span class="op">:</span>p] <span class="op">==</span><span class="st"> </span><span class="dv">0</span>)</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="logistic-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#logistic-regression" class="anchor"></a>Logistic Regression</h2>
<p>Logistic regression is a widely used method in Generalized Linear Model (GLM) to deal with binary response. Interested reader can see the Wikipedia page <a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic regression</a> for more information.</p>
<p>Let us first see the <code>logistic_regression</code> function using <code>convexjlr</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logistic_regression &lt;-<span class="st"> </span><span class="cf">function</span>(x, y){
    p &lt;-<span class="st"> </span><span class="kw">ncol</span>(x)
    ## x is a matrix and y is a vector, you have to use J(.) to send them to Julia.
    x &lt;-<span class="st"> </span><span class="kw"><a href="../reference/J.html">J</a></span>(x)
    y &lt;-<span class="st"> </span><span class="kw"><a href="../reference/J.html">J</a></span>(y)
    ## beta is the coefficient.
    beta &lt;-<span class="st"> </span><span class="kw"><a href="../reference/variable_creating.html">Variable</a></span>(p)
    ## sum(y * (x %*% beta)) - sum(logisticloss(x %*% beta))
    ## is the log-likelihood for logistic regressio,
    ## logisticloss(x) = log(1+exp(x)).
    p1 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/problem_creating.html">maximize</a></span>(<span class="kw">sum</span>(y <span class="op">*</span><span class="st"> </span>(x <span class="op">%*%</span><span class="st"> </span>beta)) <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(<span class="kw"><a href="../reference/logisticloss.html">logisticloss</a></span>(x <span class="op">%*%</span><span class="st"> </span>beta)))
    <span class="kw"><a href="../reference/cvx_optim.html">cvx_optim</a></span>(p1)
    <span class="kw"><a href="../reference/value.html">value</a></span>(beta)
}</code></pre></div>
<p>In the function, <code>x</code> is the predictor matrix, <code>y</code> is the binary response we have (we assume it to be 0-1 valued).</p>
<p>We first construct the log-likelihood of the logistic regression, and then we use <code>cvx_optim</code> to maximize it. Note that in formulating the log-likelihood, there is a little trick: we use <code><a href="../reference/logisticloss.html">logisticloss(x %*% beta)</a></code> instead of <code>log(1+exp(x %*% beta))</code>, that is because <code><a href="../reference/logisticloss.html">logisticloss(.)</a></code> is a convex function but by rule of Disciplined Convex Programming (DCP), we are not sure whether <code>log(1+exp(.))</code> is convex or not.</p>
<p>Interested readers can check <a href="http://dcp.stanford.edu/rules" class="uri">http://dcp.stanford.edu/rules</a> for more information about the rule of DCP, and use <code>?operations</code> or check <a href="http://convexjl.readthedocs.io/en/stable/operations.html" class="uri">http://convexjl.readthedocs.io/en/stable/operations.html</a> for a full list of supported operations.</p>
<p>Now we can see a little example using the <code>logistic_regression</code> function we have just built.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">500</span>
p &lt;-<span class="st"> </span><span class="dv">2</span>
## Sigma, the covariance matrix of x, is of AR-1 strcture.
Sigma &lt;-<span class="st"> </span><span class="kw">outer</span>(<span class="dv">1</span><span class="op">:</span>p, <span class="dv">1</span><span class="op">:</span>p, <span class="cf">function</span>(i, j) <span class="fl">0.5</span> <span class="op">^</span><span class="st"> </span><span class="kw">abs</span>(i <span class="op">-</span><span class="st"> </span>j))
x &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>p), n, p) <span class="op">%*%</span><span class="st"> </span><span class="kw">chol</span>(Sigma)
## the real logistic regression coefficient.
beta0 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)
## the probability for y = 1.
p0 &lt;-<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x <span class="op">%*%</span><span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span><span class="fl">0.2</span> <span class="op">*</span><span class="st"> </span><span class="kw">rnorm</span>(n)))
y &lt;-<span class="st"> </span><span class="kw">runif</span>(n) <span class="op">&lt;</span><span class="st"> </span>p0

<span class="kw">logistic_regression</span>(x, y)</code></pre></div>
<pre><code>##           [,1]
## [1,]  1.088600
## [2,] -1.162567</code></pre>
</div>
<div id="support-vector-machine" class="section level2">
<h2 class="hasAnchor">
<a href="#support-vector-machine" class="anchor"></a>Support Vector Machine</h2>
<p>Support vector machine (SVM) is a classificaiton tool. In this vignette, we just focus on the soft-margin linear SVM. Interested reader can read more about SVM in the Wikipedia page <a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support vector machine</a>.</p>
<p>Let us first see the <code>svm</code> function using <code>convexjlr</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">svm &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, lambda){
    p &lt;-<span class="st"> </span><span class="kw">ncol</span>(x)
    ## n is a scalar, you don't have to use J(.) to send it to Julia.
    n &lt;-<span class="st"> </span><span class="kw">nrow</span>(x) ## n &lt;- J(nrow(x))
    ## lambda is a scalar, you don't have to use J(.) to send it to Julia.
    ## lambda &lt;- J(lambda)
    ## x is a matrix and y is a vector, you have to use J(.) to send them to Julia.
    x &lt;-<span class="st"> </span><span class="kw"><a href="../reference/J.html">J</a></span>(x)
    y &lt;-<span class="st"> </span><span class="kw"><a href="../reference/J.html">J</a></span>(y)
    ## w and b define the classification hyperplane &lt;w.x&gt; = b.
    w &lt;-<span class="st"> </span><span class="kw"><a href="../reference/variable_creating.html">Variable</a></span>(p)
    b &lt;-<span class="st"> </span><span class="kw"><a href="../reference/variable_creating.html">Variable</a></span>()
    ## hinge_loss, note that pos(.) is the positive part function. 
    hinge_loss &lt;-<span class="st"> </span><span class="kw"><a href="../reference/Expr.html">Expr</a></span>(<span class="kw">sum</span>(<span class="kw"><a href="../reference/pos.html">pos</a></span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>y <span class="op">*</span><span class="st"> </span>(x <span class="op">%*%</span><span class="st"> </span>w <span class="op">-</span><span class="st"> </span>b))) <span class="op">/</span><span class="st"> </span>n)
    p1 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/problem_creating.html">minimize</a></span>(hinge_loss <span class="op">+</span><span class="st"> </span>lambda <span class="op">*</span><span class="st"> </span><span class="kw"><a href="../reference/sumsquares.html">sumsquares</a></span>(w))
    <span class="kw"><a href="../reference/cvx_optim.html">cvx_optim</a></span>(p1)
    <span class="kw">list</span>(<span class="dt">w =</span> <span class="kw"><a href="../reference/value.html">value</a></span>(w), <span class="dt">b =</span> <span class="kw"><a href="../reference/value.html">value</a></span>(b))
}</code></pre></div>
<p>In the function, <code>x</code> is the predictor matrix, <code>y</code> is the binary response we have (we assume it to be of negative one or one in this section). <code>lambda</code> is the positive tuning parameter which determines the tradeoff between the margin-size and classification error rate. As <code>lambda</code> becomes smaller, the classification error rate is more important. And the <code>svm</code> function will return the <code>w</code> and <code>b</code> which define the classification hyperplance as <code>&lt;w, x&gt; = b</code>.</p>
<p>Now we can see a little example using the <code>svm</code> function we have just built.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">100</span>
p &lt;-<span class="st"> </span><span class="dv">2</span>
## Sigma, the covariance matrix of x, is of AR-1 strcture.
Sigma &lt;-<span class="st"> </span><span class="kw">outer</span>(<span class="dv">1</span><span class="op">:</span>p, <span class="dv">1</span><span class="op">:</span>p, <span class="cf">function</span>(i, j) <span class="fl">0.5</span> <span class="op">^</span><span class="st"> </span><span class="kw">abs</span>(i <span class="op">-</span><span class="st"> </span>j))
## We generate two groups of points with same covariance and different mean.
x1 &lt;-<span class="st"> </span><span class="fl">0.2</span> <span class="op">*</span><span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p), n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, p) <span class="op">%*%</span><span class="st"> </span><span class="kw">chol</span>(Sigma) <span class="op">+</span><span class="st"> </span><span class="kw">outer</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>), <span class="kw">rep</span>(<span class="dv">0</span>, p))
x2 &lt;-<span class="st"> </span><span class="fl">0.2</span> <span class="op">*</span><span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>p), n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, p) <span class="op">%*%</span><span class="st"> </span><span class="kw">chol</span>(Sigma) <span class="op">+</span><span class="st"> </span><span class="kw">outer</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>), <span class="kw">rep</span>(<span class="dv">1</span>, p))
x &lt;-<span class="st"> </span><span class="kw">rbind</span>(x1, x2)
## the label for the points.
y &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>), <span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>))

r &lt;-<span class="st"> </span><span class="kw">svm</span>(x, y, <span class="fl">0.5</span>)
r</code></pre></div>
<pre><code>## $w
##            [,1]
## [1,] -0.5067705
## [2,] -0.5092681
## 
## $b
## [1] -0.4571706</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## We can scatter-plot the points and 
## draw the classification hyperplane returned by the function svm.
<span class="kw">plot</span>(x, <span class="dt">col =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">"red"</span>, n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>), <span class="kw">rep</span>(<span class="st">"blue"</span>, n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)))
<span class="kw">abline</span>(r<span class="op">$</span>b <span class="op">/</span><span class="st"> </span>r<span class="op">$</span>w[<span class="dv">2</span>], <span class="op">-</span>r<span class="op">$</span>w[<span class="dv">1</span>] <span class="op">/</span><span class="st"> </span>r<span class="op">$</span>w[<span class="dv">2</span>])</code></pre></div>
<p><img src="my-vignette_files/figure-html/unnamed-chunk-7-1.png" width="672"></p>
</div>
<div id="smallest-circle" class="section level2">
<h2 class="hasAnchor">
<a href="#smallest-circle" class="anchor"></a>Smallest Circle</h2>
<p>In the last section of the vignette, let us see an example without any background knowledge requirement.</p>
<p>Suppose we have a set of points on the plane, how can we find the smallest circle that covers all of them? By using <code>convexjlr</code>, the solution is quite straight-forward.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span> (<span class="op">!</span><span class="kw">requireNamespace</span>(<span class="st">"plotrix"</span>, <span class="dt">quietly =</span> <span class="ot">TRUE</span>)) {
    <span class="kw">stop</span>(<span class="st">"Package plotrix needed for this section of vignette to build. Please install it."</span>)
}

center &lt;-<span class="st"> </span><span class="cf">function</span>(x, y){
    ## x and y are vectors of coordinates of the points,
    ## since they are vectors, we need to use J(.) to send them to Julia.
    x &lt;-<span class="st"> </span><span class="kw"><a href="../reference/J.html">J</a></span>(x)
    y &lt;-<span class="st"> </span><span class="kw"><a href="../reference/J.html">J</a></span>(y)
    ## Suppose the center of the smallest circle is p: (p[1], p[2]).
    p &lt;-<span class="st"> </span><span class="kw"><a href="../reference/variable_creating.html">Variable</a></span>(<span class="dv">2</span>)
    ## The square of radius of the circle that covers all the points
    ## is maximum((x - p[1]) ^ 2 + (y - p[2]) ^ 2).
    r2 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/Expr.html">Expr</a></span>(<span class="kw"><a href="../reference/maximum.html">maximum</a></span>(<span class="kw"><a href="../reference/square.html">square</a></span>(x <span class="op">-</span><span class="st"> </span>p[<span class="dv">1</span>]) <span class="op">+</span><span class="st"> </span><span class="kw"><a href="../reference/square.html">square</a></span>(y <span class="op">-</span><span class="st"> </span>p[<span class="dv">2</span>])))
    ## We want to minimize r2.
    p1 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/problem_creating.html">minimize</a></span>(r2)
    <span class="kw"><a href="../reference/cvx_optim.html">cvx_optim</a></span>(p1)
    <span class="kw"><a href="../reference/value.html">value</a></span>(p)
}</code></pre></div>
<p>In the function, <code>x</code> and <code>y</code> are vectors of coordinates of the points. And the <code>center</code> function will return the coordinates of the center of the smallest circle that covers all the points.</p>
<p>Now we can see a little example using the <code>center</code> function we have just built.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">20</span>
## Generate some random points.
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n)
y &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n)

p &lt;-<span class="st"> </span><span class="kw">center</span>(x, y)
p</code></pre></div>
<pre><code>##             [,1]
## [1,] -0.42846144
## [2,]  0.07785675</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Draw the points and the smallest circle that covers all of them.
<span class="kw">plot</span>(x, y, <span class="dt">asp =</span> <span class="dv">1</span>)
plotrix<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/plotrix/topics/draw.circle">draw.circle</a></span>(p[<span class="dv">1</span>], p[<span class="dv">2</span>], <span class="dt">radius =</span> <span class="kw">sqrt</span>(<span class="kw">max</span>((x <span class="op">-</span><span class="st"> </span>p[<span class="dv">1</span>]) <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>(y <span class="op">-</span><span class="st"> </span>p[<span class="dv">2</span>]) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)))</code></pre></div>
<p><img src="my-vignette_files/figure-html/unnamed-chunk-9-1.png" width="672"></p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#lasso">Lasso</a></li>
      <li><a href="#logistic-regression">Logistic Regression</a></li>
      <li><a href="#support-vector-machine">Support Vector Machine</a></li>
      <li><a href="#smallest-circle">Smallest Circle</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Changcheng Li.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
